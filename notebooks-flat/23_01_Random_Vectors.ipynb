{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Vectors ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *vector valued random variable*, or more simply, a *random vector*, is a list of random variables defined on the same space. We will think of it as a column.\n",
    "$$\n",
    "\\mathbf{X} ~ = ~ \n",
    "\\begin{bmatrix}\n",
    "X_1 \\\\\n",
    "X_2 \\\\\n",
    "\\vdots \\\\\n",
    "X_n\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease of display, we will sometimes write $\\mathbf{X} = [X_1 X_2 \\ldots X_n]^T$ where $\\mathbf{M}^T$ is notation for the transpose of the matrix $\\mathbf{M}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *mean vector* of $\\mathbf{X}$ is $\\boldsymbol{\\mu} = [\\mu_1 \\mu_2 \\ldots \\mu_n]^T$ where $\\mu_i = E(X_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *covariance matrix* of $\\mathbf{X}$ is the $n \\times n$ matrix $\\boldsymbol{\\Sigma}$ whose $(i, j)$th element is $Cov(X_i, X_j)$. \n",
    "\n",
    "The $i$th diagonal element of $\\boldsymbol{\\Sigma}$ is the variance of $X_i$. The matrix is symmetric because of the symmetry of covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Linear Transformation: Mean Vector ###\n",
    "Let $\\mathbf{A}$ be an $m \\times n$ numerical matrix and $\\mathbf{b}$ an $m \\times 1$ numerical vector. Consider the $m \\times 1$ random vector  $\\mathbf{Y} = \\mathbf{AX} + \\mathbf{b}$. Then the $i$th element of $\\mathbf{Y}$ is \n",
    "\n",
    "$$\n",
    "Y_i ~ = ~ \\mathbf{A}_{i*}\\mathbf{X} + \\mathbf{b}_i\n",
    "$$ \n",
    "\n",
    "where $\\mathbf{A}_{i*}$ is the $i$th row of $\\mathbf{A}$ and $\\mathbf{b_i}$ is the $i$th element of $\\mathbf{b}$. Thus $Y_i$ is a linear combination of the elements of $\\mathbf{X}$. Therefore by linearity of expectation,\n",
    "\n",
    "$$\n",
    "E(Y_i) ~ = ~ \\mathbf{A}_{i*} \\boldsymbol{\\mu} + \\mathbf{b}_i\n",
    "$$\n",
    "\n",
    "Let $\\boldsymbol{\\mu}_\\mathbf{Y}$ be the mean vector of $\\mathbf{Y}$. Then by the calculation above,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu}_\\mathbf{Y} ~ = ~ \\mathbf{A} \\boldsymbol{\\mu} + \\mathbf{b}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Transformation: Covariance Matrix ###\n",
    "\n",
    "$Cov(Y_i, Y_j)$ can be calculated using bilinearity of covariance. Let $a_{ij}$ be the $(i, j)$ element of $\\mathbf{A}$. Then\n",
    "\n",
    "\\begin{align*}\n",
    "Cov(Y_i, Y_j) ~ &= ~ Cov(\\mathbf{A}_i\\mathbf{X}, \\mathbf{A}_j\\mathbf{X}) \\\\\n",
    "&= ~ Cov\\big{(} \\sum_{k=1}^m a_{ik}X_k, \\sum_{l=1}^m a_{jl}X_l \\big{)} \\\\\n",
    "&= ~ \\sum_{k=1}^m\\sum_{l=1}^m a_{ik}a_{jl}Cov(X_k, X_l) \\\\\n",
    "&= ~ \\sum_{k=1}^m\\sum_{l=1}^m a_{ik}Cov(X_k, X_l)t_{lj} ~~~~~ \\text{where } t_{lj} = \\mathbf{A}^T(l, j) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "This is the $(i, j)$th element of $\\mathbf{A}\\boldsymbol{\\Sigma}\\mathbf{A}^T$. So if $\\boldsymbol{\\Sigma}_\\mathbf{Y}$ denotes the covariance matrix $\\mathbf{Y}$, then\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Sigma}_\\mathbf{Y} ~ = ~ \\mathbf{A} \\boldsymbol{\\Sigma} \\mathbf{A}^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraints on $\\boldsymbol{\\Sigma}$ ###\n",
    "We know that $\\boldsymbol{\\Sigma}$ has to be symmetric and that all the elements on its main diagonal must be non-negative. But no matter what $\\mathbf{A}$ is, the diagonal elements of $\\boldsymbol{\\Sigma}_\\mathbf{Y}$ must all be non-negative as they are the variances of the elements of $\\mathbf{Y}$. By the formula for $\\boldsymbol{\\Sigma}_\\mathbf{Y}$ this means\n",
    "\n",
    "$$\n",
    "\\mathbf{a}^T \\boldsymbol{\\Sigma} \\mathbf{a} ~ \\ge ~ 0 ~~~~ \\text{for all } n\\times 1 \\text{ vectors } \\mathbf{a}\n",
    "$$\n",
    "\n",
    "That is, $\\boldsymbol{\\Sigma}$ must be positive semidefinite. We will be working with positive definite covariance matrices, because if $\\mathbf{a}^T \\boldsymbol{\\Sigma} \\mathbf{a} = 0$ for some $\\mathbf{a}$ then some linear combination of the elements of $\\mathbf{X}$ is constant and hence one of the elements is a linear combination of the others. \n",
    "\n",
    "In what follows, assume $\\boldsymbol{\\Sigma}$ is positive definite and hence invertible with a positive determinant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
